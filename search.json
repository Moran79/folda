[{"path":"http://iamwangsiyu.com/folda/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 folda authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"why-use-the-folda-package","dir":"Articles","previous_headings":"","what":"Why use the folda package?","title":"Introduction to folda","text":"’ve ever frustrated warnings errors MASS::lda(), appreciate ULDA implementation folda(). offers several key improvements: “constant within group” errors! ULDA can handle constant columns perfect separation groups. Automatic missing value handling! implementation seamlessly integrates automatic missing value imputation training testing phases. Fast! ULDA implemented using Generalized Singular Value Decomposition (GSVD) method, diagonalizes within-class total scatter matrices simultaneously, offering speed advantage sequential diagonalization used MASS::lda() (see Howland et al., 2003 details). also rewritten matrix decomposition modules (SVD, QR) using RcppEigen, improving computational efficiency leveraging optimized C++ code. Better visualization! folda uses ggplot2 provide visualizations class separation projected 2D spaces (1D histograms), offering valuable insights. forward LDA implementation, folda offers following advantages classical framework: issues multicollinearity perfect linear dependency! Since folda() built ULDA, effectively solves scaling matrix. Handles perfect separation offers greater power! classical approach using Wilks’ Lambda known limitations, including premature stopping () groups perfectly separated. Pillai’s trace, used folda(), effectively addresses perfect separation, also shown generally greater statistical power Wilks’ Lambda (Rencher et al., 2002).","code":""},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"basic-usage-of-folda","dir":"Articles","previous_headings":"","what":"Basic Usage of folda","title":"Introduction to folda","text":"Build ULDA model variables: Build ULDA model forward selection via Pillai’s trace: Plot results:  One-dimensional plot:  Make predictions:","code":"library(folda) mpg <- as.data.frame(ggplot2::mpg) # Prepare the data datX <- mpg[, -5] # All predictors without Y response <- mpg[, 5] # we try to predict \"cyl\" (number of cylinders) fit <- folda(datX = datX, response = response, subsetMethod = \"all\") fit <- folda(datX = datX, response = response, subsetMethod = \"forward\", testStat = \"Pillai\") print(fit) # 6 out of 11 variables are selected, displ is the most important among them #>  #> Overall Pillai's trace: 1.325 #> Associated p-value: 4.636e-74 #>  #> Prediction Results on Training Data: #> Refitting Accuracy: 0.9188 #> Gini Index: 0.7004 #>  #> Confusion Matrix: #>          Actual #> Predicted  4  5  6  8 #>         4 69  0  3  0 #>         5  8  4  2  0 #>         6  4  0 74  2 #>         8  0  0  0 68 #>  #> Group means of LD scores: #>           LD1         LD2        LD3 #> 4 -3.05298379 -0.02700248 -0.3555829 #> 5 -1.87744449  4.45014946  0.8156167 #> 6 -0.06757888 -0.28356907  0.5911862 #> 8  3.71628852  0.09697943 -0.3023424 #>  #> Forward Selection Results: #>                var statOverall   statDiff  threshold #> 1            displ    0.873393 0.87339300 0.06545381 #> 2  modelnew beetle    1.029931 0.15653777 0.05673510 #> 3       modeljetta    1.141651 0.11172064 0.05496185 #> 4 modelcaravan 2wd    1.210165 0.06851331 0.05363507 #> 5     classmidsize    1.263449 0.05328468 0.05276500 #> 6              cty    1.325255 0.06180560 0.05194279 plot(fit, datX = datX, response = response) # A 1D plot is created when there is only one feature  # or for binary classification problems. mpgSmall <- mpg[, c(\"cyl\", \"displ\")] fitSmall <- folda(mpgSmall[, -1, drop = FALSE], mpgSmall[, 1]) plot(fitSmall, mpgSmall, mpgSmall[, 1]) head(predict(fit, datX, type = \"response\")) #> [1] \"4\" \"4\" \"4\" \"4\" \"6\" \"4\" head(predict(fit, datX, type = \"prob\")) # Posterior probabilities #>           4            5            6            8 #> 1 0.9966769 7.475058e-08 0.0033230408 7.023764e-12 #> 2 0.9994438 1.401133e-08 0.0005562131 5.338710e-13 #> 3 0.9970911 3.835722e-08 0.0029088506 1.738154e-11 #> 4 0.9983963 2.196016e-08 0.0016037009 7.365641e-12 #> 5 0.3122116 6.809673e-07 0.6877815595 6.173116e-06 #> 6 0.5995781 4.275271e-07 0.4004193019 2.123291e-06"},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"comparison-of-pillais-trace-and-wilks-lambda","dir":"Articles","previous_headings":"","what":"Comparison of Pillai’s Trace and Wilks’ Lambda","title":"Introduction to folda","text":", compare performances scenario one group classes perfectly separable another, condition Wilks’ Lambda performs poorly. Now let’s predict model car. Wilks’ Lambda selects manufacturer-audi, since can separate a4, a4 quattro, a6 quattro models. However, unexpectedly stops since Wilks’ Lambda = 0, leading refitting accuracy 0.0812. hand, Pillai’s trace selects 26 variables total refitting accuracy 0.9231. Additionally, MASS::lda() throw error scenario due “constant within groups” issue.","code":"fitW <- folda(mpg[, -2], mpg[, 2], testStat = \"Wilks\") fitW$forwardInfo #>                var statOverall statDiff threshold #> 1 manufactureraudi           0        0 0.7338759 fitP <- folda(mpg[, -2], mpg[, 2], testStat = \"Pillai\") fitP$forwardInfo #>                       var statOverall  statDiff threshold #> 1       manufacturerdodge     1.00000 1.0000000 0.2654136 #> 2  manufacturervolkswagen     2.00000 1.0000000 0.2599086 #> 3        manufactureraudi     3.00000 1.0000000 0.2543730 #> 4        manufacturerford     4.00000 1.0000000 0.2488055 #> 5   manufacturerchevrolet     5.00000 1.0000000 0.2432051 #> 6       manufacturerhonda     6.00000 1.0000000 0.2375706 #> 7     manufacturerhyundai     7.00000 1.0000000 0.2319008 #> 8        manufacturerjeep     8.00000 1.0000000 0.2261943 #> 9  manufacturerland rover     9.00000 1.0000000 0.2204496 #> 10    manufacturerlincoln    10.00000 1.0000000 0.2146652 #> 11    manufacturermercury    11.00000 1.0000000 0.2088394 #> 12     manufacturernissan    12.00000 1.0000000 0.2029702 #> 13    manufacturerpontiac    13.00000 1.0000000 0.1970556 #> 14     manufacturersubaru    14.00000 1.0000000 0.1910933 #> 15                   drvf    15.00000 1.0000000 0.1850810 #> 16                   drvr    16.00000 1.0000000 0.1785056 #> 17           classminivan    17.00000 1.0000000 0.1723677 #> 18            classpickup    18.00000 1.0000000 0.1661696 #> 19               classsuv    19.00000 1.0000000 0.1599071 #> 20           classmidsize    19.93159 0.9315947 0.1535761 #> 21        classsubcompact    20.74392 0.8123229 0.1475693 #> 22           classcompact    21.71027 0.9663480 0.1421899 #> 23                  displ    21.96954 0.2592742 0.1358348 #> 24          transauto(s5)    22.16831 0.1987676 0.1335988 #> 25                    cty    22.35530 0.1869879 0.1316772 #> 26                    flp    22.52155 0.1662562 0.1297761 # MASS::lda(model~., data = mpg)  #> Error in lda.default(x, grouping, ...) :  #>   variables  1  2  3  4  5  6  7  8  9 10 11 12 13 14 27 28 37 38 40 appear to be constant within groups"},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"handling-missing-values","dir":"Articles","previous_headings":"","what":"Handling Missing Values","title":"Introduction to folda","text":"default method handle missing values c(medianFlag, newLevel). means numerical variables, missing values imputed median, categorical variables, new level assigned represent missing values. Additionally, numerical variables, generate missing value indicators flag observations missing data. Two key functions involved process missingFix() getDataInShape(): missingFix() imputes missing values outputs two objects: imputed dataset missing reference, can used future imputations. constant columns remains imputed dataset removed. getDataInShape() takes new data missing reference inputs, returns imputed dataset. function performs several tasks: Redundant column removal: columns new data present reference removed. Missing column addition: Columns present reference missing new data added initialized according missing reference. Flag variable handling: Missing value indicators (flag variables) properly updated reflect missing values new data. Factor level updating: categorical variables, factor levels updated match reference. factor variable new data contains levels present reference, levels removed, values set match reference. Redundant levels also removed. Impute missing values create missing reference: X1 X2 removed constant (.e., values NA). X3 X4 remain unchanged. X5 imputed median (3), new column X5_FLAG added indicate missing values. X6 imputed new level ‘new0_0Level’. Now, let’s create new dataset imputation. Apply missing reference new dataset: X1 removed exist missing reference. X3 remains unchanged. “F” new level X4, removed imputed “” (frequent level) along missing values. X5 imputed, new column X5_FLAG added indicate missing values. X6 missing new data, initialized level “new0_0Level”. Next, show example using folda airquality dataset. First, let’s check columns airquality missing values: response variable 5th column (Month): generated missing reference : make prediction: Notice issues arise predicting, even new data contains nothing missing values.","code":"# Create a dataset with missing values (datNA <- data.frame(X1 = rep(NA, 5), # All values are NA                      X2 = factor(rep(NA, 5), levels = LETTERS[1:3]), # Factor with all NA values                      X3 = 1:5, # Numeric column with no missing values                      X4 = LETTERS[1:5], # Character column                      X5 = c(NA, 2, 3, 10, NA), # Numeric column with missing values                      X6 = factor(c(\"A\", NA, NA, \"B\", \"B\"), levels = LETTERS[1:3]))) # Factor with missing values #>   X1   X2 X3 X4 X5   X6 #> 1 NA <NA>  1  A NA    A #> 2 NA <NA>  2  B  2 <NA> #> 3 NA <NA>  3  C  3 <NA> #> 4 NA <NA>  4  D 10    B #> 5 NA <NA>  5  E NA    B (imputedSummary <- missingFix(datNA)) #> $data #>   X3 X4 X5          X6 X5_FLAG #> 1  1  A  3           A       1 #> 2  2  B  2 new0_0Level       0 #> 3  3  C  3 new0_0Level       0 #> 4  4  D 10           B       0 #> 5  5  E  3           B       1 #>  #> $ref #>   X3 X4 X5          X6 X5_FLAG #> 1  3  A  3 new0_0Level       1 (datNAnew <- data.frame(X1 = 1:3, # New column not in the reference                         X3 = 1:3, # Matching column with no NAs                         X4 = as.factor(c(\"E\", \"F\", NA)), # Factor with a new level \"F\" and missing values                         X5 = c(NA, 2, 3))) # Numeric column with a missing value #>   X1 X3   X4 X5 #> 1  1  1    E NA #> 2  2  2    F  2 #> 3  3  3 <NA>  3 getDataInShape(datNAnew, imputedSummary$ref) #>   X3 X4 X5          X6 X5_FLAG #> 1  1  E  3 new0_0Level       1 #> 2  2  A  2 new0_0Level       0 #> 3  3  A  3 new0_0Level       0 sapply(airquality, anyNA) # Ozone and Solar.R have NAs #>   Ozone Solar.R    Wind    Temp   Month     Day  #>    TRUE    TRUE   FALSE   FALSE   FALSE   FALSE fitAir <- folda(airquality[, -5], airquality[, 5]) fitAir$misReference #>   Ozone Solar.R Wind Temp Day Ozone_FLAG Solar.R_FLAG #> 1  31.5     205  9.7   79  16          1            1 predict(fitAir, data.frame(rep(NA, 4))) #> [1] \"6\" \"6\" \"6\" \"6\""},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"downsampling","dir":"Articles","previous_headings":"","what":"Downsampling","title":"Introduction to folda","text":"two common scenarios downsampling can helpful: classes highly imbalanced, downsampling can make balanced. speed computation. ULDA classifier computed based class centroids covariance structure. sufficient number data points, 3000, included, additional data points may minimal impact. default, downsampling disabled. downsampling = TRUE kSample = NULL, downsample classes size smallest class. kSample specified, classes downsampled maximum kSample samples. class contains fewer kSample samples, observations class retained. Suppose want predict number cylinders (cyl) car. number observations class : apply downsampling without specifying kSample, randomly select 4 samples group, smallest group (cyl = 5) 4 observations. can also set kSample = 30. case, 30 random samples selected cyl = 4, 6, 8, 4 samples chosen cyl = 5. ’s important note downsampling process changes prior, subsequent results based downsampled data. want downsample speed (another reason maintaining original proportion prior), sure specify prior explicitly. can see, prior model suppresses prediction class cyl = 5, confusion matrix differs one fitCyl.","code":"table(mpg$cyl) #>  #>  4  5  6  8  #> 81  4 79 70 set.seed(443) fitCyl <- folda(mpg[, -5], mpg[, 5], downSampling = TRUE) fitCyl$confusionMatrix #>          Actual #> Predicted 4 5 6 8 #>         4 4 0 0 0 #>         5 0 4 0 0 #>         6 0 0 4 0 #>         8 0 0 0 4 fitCyl30 <- folda(mpg[, -5], mpg[, 5], downSampling = TRUE, kSample = 30) fitCyl30$confusionMatrix #>          Actual #> Predicted  4  5  6  8 #>         4 22  0  3  0 #>         5  8  4  2  0 #>         6  0  0 25  2 #>         8  0  0  0 28 fitCylWithPrior <- folda(mpg[, -5], mpg[, 5], downSampling = TRUE, prior = table(mpg[, 5])) fitCylWithPrior$confusionMatrix #>          Actual #> Predicted 4 5 6 8 #>         4 4 4 1 0 #>         5 0 0 0 0 #>         6 0 0 3 0 #>         8 0 0 0 4"},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"additional-features","dir":"Articles","previous_headings":"","what":"Additional Features","title":"Introduction to folda","text":"correction: ’re less concerned controlling type error prefer aggressive variable selection process, setting correction = FALSE may result better testing accuracy, particularly number columns exceeds number rows. alpha: goal rank variables, set alpha = 1. ensures variables filtered selection process. misClassCost: parameter useful situations misclassifying certain classes severe impact compared others. example demonstrating incorporate different misclassification costs. iris dataset famous dataset three species flowers: Suppose misclassifying versicolor species costly. potential misclassification cost matrix might look like : means misclassifying versicolor species 100 times severe misclassifying species versicolor. First, let’s fit model equal misclassification costs specified misclassification costs: prediction distributions equal misclassification costs: prediction distributions specified misclassification costs: shown, model tends predict versicolor often due higher misclassification cost associated predicting incorrectly.","code":"table(iris$Species, dnn = NULL) #>     setosa versicolor  virginica  #>         50         50         50 misClassCost <- matrix(c(0, 100, 1,                          1, 0, 1,                          1, 100, 0), 3, 3, byrow = TRUE) fitEqualCost <- folda(iris[, -5], response = iris[, 5]) fitNewCost <- folda(iris[, -5], response = iris[, 5], misClassCost = misClassCost) table(predict(fitEqualCost, iris), dnn = NULL) #>     setosa versicolor  virginica  #>         50         49         51 table(predict(fitNewCost, iris), dnn = NULL) #>     setosa versicolor  virginica  #>         50         63         37"},{"path":"http://iamwangsiyu.com/folda/articles/folda.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Introduction to folda","text":"Howland, P., Jeon, M., & Park, H. (2003). Structure preserving dimension reduction clustered text data based generalized singular value decomposition. SIAM Journal Matrix Analysis Applications, 25(1), 165-179. Rencher, . C., & Christensen, W. F. (2002). Methods Multivariate Analysis (Vol. 727). John Wiley & Sons. Wang, S. (2024). new forward discriminant analysis framework based Pillai’s trace ULDA. arXiv preprint, arXiv:2409.03136. Retrieved https://arxiv.org/abs/2409.03136.","code":""},{"path":"http://iamwangsiyu.com/folda/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Siyu Wang. Author, maintainer, copyright holder.","code":""},{"path":"http://iamwangsiyu.com/folda/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wang S (2024). folda: Forward Stepwise Discriminant Analysis Pillai's Trace. R package version 0.2.0.9000, http://iamwangsiyu.com/folda/, https://github.com/Moran79/folda.","code":"@Manual{,   title = {folda: Forward Stepwise Discriminant Analysis with Pillai's Trace},   author = {Siyu Wang},   year = {2024},   note = {R package version 0.2.0.9000, http://iamwangsiyu.com/folda/},   url = {https://github.com/Moran79/folda}, }"},{"path":"http://iamwangsiyu.com/folda/index.html","id":"folda-","dir":"","previous_headings":"","what":"Forward Stepwise Discriminant Analysis with Pillai's Trace","title":"Forward Stepwise Discriminant Analysis with Pillai's Trace","text":"folda package R modeling tool designed fitting Forward Stepwise Linear Discriminant Analysis (LDA) Uncorrelated Linear Discriminant Analysis (ULDA). ’re unfamiliar stepwise LDA ULDA, please refer following resources: stepwise LDA using Wilks’ Lambda, see Section 6.11.1 Methods Multivariate Analysis, Third Edition Alvin C. Rencher William F. Christensen (2012). ULDA, refer Ye, J., & Yu, B. (2005). Characterization family algorithms generalized discriminant analysis undersampled problems. Journal Machine Learning Research, 6(4). Link. combination ULDA forward LDA using Pillai’s trace, see Wang, S. (2024). New Forward Discriminant Analysis Framework Based Pillai’s Trace ULDA. arXiv preprint arXiv:2409.03136. Link.","code":""},{"path":"http://iamwangsiyu.com/folda/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Forward Stepwise Discriminant Analysis with Pillai's Trace","text":"can install development version folda GitHub :","code":"install.packages(\"folda\") # install.packages(\"devtools\") devtools::install_github(\"Moran79/folda\")"},{"path":"http://iamwangsiyu.com/folda/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Forward Stepwise Discriminant Analysis with Pillai's Trace","text":"’ve ever frustrated warnings errors MASS::lda(), appreciate ULDA implementation folda(). offers several key improvements: “constant within group” errors! ULDA can handle constant columns perfect separation groups. Automatic missing value handling! implementation seamlessly integrates automatic missing value imputation training testing phases. Fast! ULDA implemented using Generalized Singular Value Decomposition (GSVD) method, diagonalizes within-class total scatter matrices simultaneously, offering speed advantage sequential diagonalization used MASS::lda() (see Howland et al., 2003 details). also rewritten matrix decomposition modules (SVD, QR) using RcppEigen, improving computational efficiency leveraging optimized C++ code. Better visualization! folda uses ggplot2 provide visualizations class separation projected 2D spaces (1D histograms), offering valuable insights. forward LDA implementation, folda offers following advantages classical framework: issues multicollinearity perfect linear dependency! Handles perfect separation offers greater power! classical approach using Wilks’ Lambda known limitations, including premature stopping () groups perfectly separated. Pillai’s trace, used folda(), effectively addresses perfect separation, also shown generally greater statistical power Wilks’ Lambda (Rencher et al., 2002).","code":""},{"path":"http://iamwangsiyu.com/folda/index.html","id":"basic-usage","dir":"","previous_headings":"","what":"Basic Usage","title":"Forward Stepwise Discriminant Analysis with Pillai's Trace","text":"Build ULDA model variables: Build ULDA model forward selection via Pillai’s trace: Plot results:  One-dimensional plot:  Make predictions: examples can found vignette.","code":"library(folda) mpg <- as.data.frame(ggplot2::mpg) # Prepare the data datX <- mpg[, -5] # All predictors without Y response <- mpg[, 5] # we try to predict \"cyl\" (number of cylinders) fit <- folda(datX = datX, response = response, subsetMethod = \"all\") fit <- folda(datX = datX, response = response, subsetMethod = \"forward\", testStat = \"Pillai\") print(fit) # 6 out of 11 variables are selected, displ is the most important among them #>  #> Overall Pillai's trace: 1.325 #> Associated p-value: 4.636e-74 #>  #> Prediction Results on Training Data: #> Refitting Accuracy: 0.9188 #> Gini Index: 0.7004 #>  #> Confusion Matrix: #>          Actual #> Predicted  4  5  6  8 #>         4 69  0  3  0 #>         5  8  4  2  0 #>         6  4  0 74  2 #>         8  0  0  0 68 #>  #> Group means of LD scores: #>           LD1         LD2        LD3 #> 4  3.05298379  0.02700248 -0.3555829 #> 5  1.87744449 -4.45014946  0.8156167 #> 6  0.06757888  0.28356907  0.5911862 #> 8 -3.71628852 -0.09697943 -0.3023424 #>  #> Forward Selection Results: #>                var statOverall   statDiff  threshold #> 1            displ    0.873393 0.87339300 0.06545381 #> 2  modelnew beetle    1.029931 0.15653777 0.05673510 #> 3       modeljetta    1.141651 0.11172064 0.05496185 #> 4 modelcaravan 2wd    1.210165 0.06851331 0.05363507 #> 5     classmidsize    1.263449 0.05328468 0.05276500 #> 6              cty    1.325255 0.06180560 0.05194279 plot(fit, datX = datX, response = response) # A 1D plot is created when there is only one feature  # or for binary classification problems. mpgSmall <- mpg[, c(\"cyl\", \"displ\")] fitSmall <- folda(mpgSmall[, -1, drop = FALSE], mpgSmall[, 1]) plot(fitSmall, mpgSmall, mpgSmall[, 1]) head(predict(fit, datX, type = \"response\")) #> [1] \"4\" \"4\" \"4\" \"4\" \"6\" \"4\" head(predict(fit, datX, type = \"prob\")) # Posterior probabilities #>           4            5            6            8 #> 1 0.9966769 7.475058e-08 0.0033230408 7.023764e-12 #> 2 0.9994438 1.401133e-08 0.0005562131 5.338710e-13 #> 3 0.9970911 3.835722e-08 0.0029088506 1.738154e-11 #> 4 0.9983963 2.196016e-08 0.0016037009 7.365641e-12 #> 5 0.3122116 6.809673e-07 0.6877815595 6.173116e-06 #> 6 0.5995781 4.275271e-07 0.4004193019 2.123291e-06"},{"path":"http://iamwangsiyu.com/folda/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Forward Stepwise Discriminant Analysis with Pillai's Trace","text":"Howland, P., Jeon, M., & Park, H. (2003). Structure preserving dimension reduction clustered text data based generalized singular value decomposition. SIAM Journal Matrix Analysis Applications, 25(1), 165-179. Rencher, . C., & Christensen, W. F. (2002). Methods Multivariate Analysis (Vol. 727). John Wiley & Sons. Wang, S. (2024). new forward discriminant analysis framework based Pillai’s trace ULDA. arXiv preprint, arXiv:2409.03136. Retrieved https://arxiv.org/abs/2409.03136.","code":""},{"path":"http://iamwangsiyu.com/folda/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting help","title":"Forward Stepwise Discriminant Analysis with Pillai's Trace","text":"encounter clear bug, please file issue minimal reproducible example GitHub","code":""},{"path":"http://iamwangsiyu.com/folda/reference/checkPriorAndMisClassCost.html","id":null,"dir":"Reference","previous_headings":"","what":"Check and Normalize Prior Probabilities and Misclassification Costs — checkPriorAndMisClassCost","title":"Check and Normalize Prior Probabilities and Misclassification Costs — checkPriorAndMisClassCost","text":"function verifies normalizes provided prior probabilities misclassification cost matrix given response variable. ensures lengths prior dimensions misclassification cost matrix match number levels response variable. prior misClassCost provided, default values used: prior set observed frequencies response, misclassification cost matrix set 1 misclassifications 0 correct classifications.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/checkPriorAndMisClassCost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check and Normalize Prior Probabilities and Misclassification Costs — checkPriorAndMisClassCost","text":"","code":"checkPriorAndMisClassCost(prior, misClassCost, response)"},{"path":"http://iamwangsiyu.com/folda/reference/checkPriorAndMisClassCost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check and Normalize Prior Probabilities and Misclassification Costs — checkPriorAndMisClassCost","text":"prior numeric vector representing prior probabilities class response variable. NULL, observed frequencies response used default prior. misClassCost square matrix representing misclassification costs pair classes response variable. NULL, default misclassification matrix created misclassifications cost 1 correct classifications cost 0. response factor representing response variable multiple classes.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/checkPriorAndMisClassCost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check and Normalize Prior Probabilities and Misclassification Costs — checkPriorAndMisClassCost","text":"list containing: prior normalized vector prior probabilities class. misClassCost square matrix representing misclassification costs, rows columns labeled levels response variable.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/checkPriorAndMisClassCost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check and Normalize Prior Probabilities and Misclassification Costs — checkPriorAndMisClassCost","text":"","code":"# Example 1: Using default prior and misClassCost response <- factor(c('A', 'B', 'A', 'B', 'C', 'A')) checkPriorAndMisClassCost(NULL, NULL, response) #> $prior #>         A         B         C  #> 0.5000000 0.3333333 0.1666667  #>  #> $misClassCost #>   A B C #> A 0 1 1 #> B 1 0 1 #> C 1 1 0 #>   # Example 2: Providing custom prior and misClassCost prior <- c(A = 1, B = 1, C = 2) misClassCost <- matrix(c(0, 2, 10,                          1, 0, 10,                          1, 2, 0), nrow = 3, byrow = TRUE) checkPriorAndMisClassCost(prior, misClassCost, response) #> $prior #>    A    B    C  #> 0.25 0.25 0.50  #>  #> $misClassCost #>   A B  C #> A 0 2 10 #> B 1 0 10 #> C 1 2  0 #>"},{"path":"http://iamwangsiyu.com/folda/reference/folda.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward Uncorrelated Linear Discriminant Analysis — folda","title":"Forward Uncorrelated Linear Discriminant Analysis — folda","text":"function fits ULDA (Uncorrelated Linear Discriminant Analysis) model provided data, option forward selection variables based Pillai's trace Wilks' Lambda. can also handle missing values, perform downsampling, compute linear discriminant scores group means classification. function returns fitted ULDA model object.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/folda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward Uncorrelated Linear Discriminant Analysis — folda","text":"","code":"folda(   datX,   response,   subsetMethod = c(\"forward\", \"all\"),   testStat = c(\"Pillai\", \"Wilks\"),   correction = TRUE,   alpha = 0.1,   prior = NULL,   misClassCost = NULL,   missingMethod = c(\"medianFlag\", \"newLevel\"),   downSampling = FALSE,   kSample = NULL )"},{"path":"http://iamwangsiyu.com/folda/reference/folda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward Uncorrelated Linear Discriminant Analysis — folda","text":"datX data frame predictor variables. response factor representing response variable multiple classes. subsetMethod character string specifying method variable selection. Options \"forward\" forward selection \"\" using variables. Default \"forward\". testStat character string specifying test statistic use forward selection. Options \"Pillai\" \"Wilks\". Default \"Pillai\". correction logical value indicating whether apply multiple comparison correction forward selection. Default TRUE. alpha numeric value 0 1 specifying significance level test statistic forward selection. Default 0.1. prior numeric vector representing prior probabilities class response variable. NULL, observed class frequencies used prior. Default NULL. misClassCost square matrix \\(C\\), element \\(C_{ij}\\) represents cost classifying observation class \\(\\) given truly belongs class \\(j\\). NULL, default matrix equal misclassification costs class pairs used. Default NULL. missingMethod character vector length 2 specifying handle missing values numerical categorical variables, respectively. Default c(\"medianFlag\", \"newLevel\"). downSampling logical value indicating whether perform downsampling balance class distribution training data improve computational efficiency. Default FALSE. Note downsampling applied prior NULL, class prior calculated based downsampled data. retain original prior, please specify explicitly using prior parameter. kSample integer specifying maximum number samples take class downsampling. NULL, number samples limited size smallest class. Default NULL.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/folda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forward Uncorrelated Linear Discriminant Analysis — folda","text":"list class ULDA containing following components: scaling matrix scaling coefficients linear discriminants. groupMeans group means linear discriminant scores. prior prior probabilities class. misClassCost misclassification cost matrix. misReference reference handling missing values. terms terms used model formula. xlevels levels factors used model. varIdx indices selected variables. varSD standard deviations selected variables. varCenter means selected variables. statPillai Pillai's trace statistic. pValue p-value associated Pillai's trace. predGini Gini index predictions training data. confusionMatrix confusion matrix training data predictions. forwardInfo Information forward selection process, applicable. stopInfo message indicating forward selection stopped, applicable.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/folda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forward Uncorrelated Linear Discriminant Analysis — folda","text":"Howland, P., Jeon, M., & Park, H. (2003). Structure preserving dimension reduction clustered text data based generalized singular value decomposition. SIAM Journal Matrix Analysis Applications Wang, S. (2024). New Forward Discriminant Analysis Framework Based Pillai's Trace ULDA. arXiv preprint arXiv:2409.03136. Available https://arxiv.org/abs/2409.03136.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/folda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forward Uncorrelated Linear Discriminant Analysis — folda","text":"","code":"# Fit the ULDA model fit <- folda(datX = iris[, -5], response = iris[, 5], subsetMethod = \"all\")  # Fit the ULDA model with forward selection fit <- folda(datX = iris[, -5], response = iris[, 5], subsetMethod = \"forward\")"},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Chi-Squared Statistics for Variables — getChiSqStat","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"function calculates chi-squared statistic column datX response variable response. supports numerical categorical predictors datX. numerical variables, automatically discretizes factor levels based standard deviations mean, using different splitting criteria depending sample size.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"","code":"getChiSqStat(datX, response)"},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"datX matrix data frame containing predictor variables. can consist numerical categorical variables. response factor representing class labels. must least two levels chi-squared test applicable.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"vector chi-squared statistics, one predictor variable datX. numerical variables, chi-squared statistic computed binning variable.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"variable datX, function first checks variable numerical. , discretized factor levels using either two three split points, depending sample size number levels response. Missing values handled assigning new factor level. chi-squared statistic computed predictor response. chi-squared test one degree freedom, Wilson-Hilferty transformation applied adjust statistic 1-degree--freedom chi-squared distribution.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"Loh, W. Y. (2009). Improving precision classification trees. Annals Applied Statistics, 1710–1737. JSTOR.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getChiSqStat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Chi-Squared Statistics for Variables — getChiSqStat","text":"","code":"datX <- data.frame(var1 = rnorm(100), var2 = factor(sample(letters[1:3], 100, replace = TRUE))) y <- factor(sample(c(\"A\", \"B\"), 100, replace = TRUE)) getChiSqStat(datX, y) #>      var1      var2  #> 0.5632381 1.3431827"},{"path":"http://iamwangsiyu.com/folda/reference/getDataInShape.html","id":null,"dir":"Reference","previous_headings":"","what":"Align Data with a Missing Reference — getDataInShape","title":"Align Data with a Missing Reference — getDataInShape","text":"function aligns given dataset (data) reference dataset (missingReference). ensures structure, column names, factor levels data match structure missingReference. necessary, missing columns initialized NA, factor levels adjusted match reference. Additionally, handles imputation missing values based reference manages flag variables categorical numerical columns.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getDataInShape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align Data with a Missing Reference — getDataInShape","text":"","code":"getDataInShape(data, missingReference)"},{"path":"http://iamwangsiyu.com/folda/reference/getDataInShape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align Data with a Missing Reference — getDataInShape","text":"data data frame aligned adjusted according missingReference. missingReference reference data frame provides structure (column names, factor levels, missing value reference) aligning data.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getDataInShape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Align Data with a Missing Reference — getDataInShape","text":"data frame structure, column names, factor levels data aligned missingReference. Missing values data imputed based first row missingReference, flag variables updated accordingly.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getDataInShape.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Align Data with a Missing Reference — getDataInShape","text":"","code":"data <- data.frame(   X1_FLAG = c(0, 0, 0),   X1 = factor(c(NA, \"C\", \"B\"), levels = LETTERS[2:3]),   X2_FLAG = c(NA, 0, 1),   X2 = c(2, NA, 3) )  missingReference <- data.frame(   X1_FLAG = 1,   X1 = factor(\"A\", levels = LETTERS[1:2]),   X2 = 1,   X2_FLAG = 1 )  getDataInShape(data, missingReference) #>   X1_FLAG X1 X2 X2_FLAG #> 1       1  A  2       0 #> 2       1  A  1       1 #> 3       0  B  3       1"},{"path":"http://iamwangsiyu.com/folda/reference/getMode.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Mode of a Factor Variable with Optional Priors — getMode","title":"Calculate the Mode of a Factor Variable with Optional Priors — getMode","text":"function calculates mode given factor vector can coerced factor. can optionally provide prior weights level factor.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getMode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Mode of a Factor Variable with Optional Priors — getMode","text":"","code":"getMode(v, prior)"},{"path":"http://iamwangsiyu.com/folda/reference/getMode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Mode of a Factor Variable with Optional Priors — getMode","text":"v factor vector can coerced factor. mode calculated levels factor. prior numeric vector prior weights level factor. provided, levels given equal weight.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getMode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Mode of a Factor Variable with Optional Priors — getMode","text":"mode factor v character string. values NA, function returns NA.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/getMode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Mode of a Factor Variable with Optional Priors — getMode","text":"","code":"# Example 1: Mode without priors v <- factor(c(\"apple\", \"banana\", \"apple\", \"orange\", NA)) getMode(v) #> [1] \"apple\"  # Example 2: Mode with priors v <- factor(c(\"apple\", \"banana\", \"apple\", \"orange\", NA)) prior <- c(apple = 0.5, banana = 1.5, orange = 1) getMode(v, prior) #> [1] \"banana\""},{"path":"http://iamwangsiyu.com/folda/reference/missingFix.html","id":null,"dir":"Reference","previous_headings":"","what":"Impute Missing Values and Add Missing Flags to a Data Frame — missingFix","title":"Impute Missing Values and Add Missing Flags to a Data Frame — missingFix","text":"function imputes missing values data frame based specified methods numerical categorical variables. Additionally, can add flag columns indicate missing values. numerical variables, missing values can imputed using mean median. categorical variables, missing values can imputed using mode new level. function also removes constant columns (NAs observed value).","code":""},{"path":"http://iamwangsiyu.com/folda/reference/missingFix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Impute Missing Values and Add Missing Flags to a Data Frame — missingFix","text":"","code":"missingFix(data, missingMethod = c(\"medianFlag\", \"newLevel\"))"},{"path":"http://iamwangsiyu.com/folda/reference/missingFix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Impute Missing Values and Add Missing Flags to a Data Frame — missingFix","text":"data data frame containing data processed. Missing values (NA) imputed based methods provided missingMethod. missingMethod character vector length 2 specifying methods imputing missing values. first element specifies method numerical variables (\"mean\", \"median\", \"meanFlag\", \"medianFlag\"), second element specifies method categorical variables (\"mode\", \"modeFlag\", \"newLevel\"). \"Flag\" included, flag column added corresponding variable type.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/missingFix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Impute Missing Values and Add Missing Flags to a Data Frame — missingFix","text":"list two elements: data original data frame missing values imputed, flag columns added applicable. ref reference row containing imputed values flag levels, can used future predictions reference.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/missingFix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Impute Missing Values and Add Missing Flags to a Data Frame — missingFix","text":"","code":"dat <- data.frame(   X1 = rep(NA, 5),   X2 = factor(rep(NA, 5), levels = LETTERS[1:3]),   X3 = 1:5,   X4 = LETTERS[1:5],   X5 = c(NA, 2, 3, 10, NA),   X6 = factor(c(\"A\", NA, NA, \"B\", \"B\"), levels = LETTERS[1:3]) ) missingFix(dat) #> $data #>   X3 X4 X5          X6 X5_FLAG #> 1  1  A  3           A       1 #> 2  2  B  2 new0_0Level       0 #> 3  3  C  3 new0_0Level       0 #> 4  4  D 10           B       0 #> 5  5  E  3           B       1 #>  #> $ref #>   X3 X4 X5          X6 X5_FLAG #> 1  3  A  3 new0_0Level       1 #>"},{"path":"http://iamwangsiyu.com/folda/reference/plot.ULDA.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Decision Boundaries and Linear Discriminant Scores — plot.ULDA","title":"Plot Decision Boundaries and Linear Discriminant Scores — plot.ULDA","text":"function plots decision boundaries linear discriminant (LD) scores given ULDA model. binary classification problem, density plot created. Otherwise, scatter plot decision boundaries generated.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/plot.ULDA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Decision Boundaries and Linear Discriminant Scores — plot.ULDA","text":"","code":"# S3 method for class 'ULDA' plot(x, datX, response, ...)"},{"path":"http://iamwangsiyu.com/folda/reference/plot.ULDA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Decision Boundaries and Linear Discriminant Scores — plot.ULDA","text":"x fitted ULDA model object. datX data frame containing predictor variables. response factor representing response variable (training labels) corresponding datX. ... Additional arguments.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/plot.ULDA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Decision Boundaries and Linear Discriminant Scores — plot.ULDA","text":"ggplot2 plot object, either density plot scatter plot decision boundaries.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/plot.ULDA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Decision Boundaries and Linear Discriminant Scores — plot.ULDA","text":"","code":"fit <- folda(datX = iris[, -5], response = iris[, 5], subsetMethod = \"all\") plot(fit, iris[, -5], iris[, 5])"},{"path":"http://iamwangsiyu.com/folda/reference/predict.ULDA.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for ULDA Model — predict.ULDA","title":"Predict Method for ULDA Model — predict.ULDA","text":"function predicts class labels class probabilities new data using fitted ULDA model. prediction can return either likely class (\"response\") posterior probabilities class (\"prob\").","code":""},{"path":"http://iamwangsiyu.com/folda/reference/predict.ULDA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for ULDA Model — predict.ULDA","text":"","code":"# S3 method for class 'ULDA' predict(object, newdata, type = c(\"response\", \"prob\"), ...)"},{"path":"http://iamwangsiyu.com/folda/reference/predict.ULDA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for ULDA Model — predict.ULDA","text":"object fitted ULDA model object. newdata data frame containing new predictor variables predictions made. type character string specifying type prediction return. \"response\" returns predicted class labels, \"prob\" returns posterior probabilities class. Default \"response\". ... Additional arguments.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/predict.ULDA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for ULDA Model — predict.ULDA","text":"type = \"response\", function returns vector predicted class labels. type = \"prob\", returns matrix posterior probabilities, row corresponds sample column class.","code":""},{"path":"http://iamwangsiyu.com/folda/reference/predict.ULDA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for ULDA Model — predict.ULDA","text":"","code":"fit <- folda(datX = iris[, -5], response = iris[, 5], subsetMethod = \"all\")  # Predict class labels predictions <- predict(fit, iris, type = \"response\")  # Predict class probabilities prob_predictions <- predict(fit, iris, type = \"prob\")"},{"path":[]},{"path":"http://iamwangsiyu.com/folda/news/index.html","id":"folda-020","dir":"Changelog","previous_headings":"","what":"folda 0.2.0","title":"folda 0.2.0","text":"CRAN release: 2024-10-29 Update examples README Vignette. Fallback base::svd users latest C++ Eigen module. Remove redundant help manual. Export functions needed LDATree package. Update algorithm: Introduce Chi-Squared variable selection significant variables.","code":""},{"path":"http://iamwangsiyu.com/folda/news/index.html","id":"folda-010","dir":"Changelog","previous_headings":"","what":"folda 0.1.0","title":"folda 0.1.0","text":"CRAN release: 2024-09-11 Initial CRAN submission.","code":""}]
